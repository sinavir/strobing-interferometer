{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stroboscopic imaging system","text":"<p>This is the documentation of the stroboscopic imaging system. It includes all the necessary knowledge to operate the system.</p>"},{"location":"#contact","title":"Contact","text":"<p>If you have any question please do not hesitate to send an email to us (the two masters who did this project):</p> <ul> <li>Maurice Debray: <code>maurice.debray (at) ens.fr</code></li> <li>TODO email fred</li> </ul>"},{"location":"getting_started/","title":"Getting started","text":"<p>This is a recipe on how to make an acquisition. We recommend reading optical setup presentation page before trying to use the setup.</p>"},{"location":"getting_started/#load-the-membrane","title":"Load the membrane","text":"<p>Load the membrane and turn on the laser (follow more detailed instruction provided in the experimental setup section)</p>"},{"location":"getting_started/#start-the-software","title":"Start the software","text":"<ol> <li>On the QMPL desktop start LabOne and open jupyter notebook provided at    <code>TODO</code> in the <code>work</code> Conda environment (this is the usual environment).</li> <li>Launch the ThorCam software and open the pinhole camera <code>TODO SN pinhole camera</code></li> <li>Run the first cell of the notebook to initialize all the code (some initial    settings are tunable. Refer to the reference here for more details)</li> <li>Fill the folder to store the data in the next cell</li> </ol> <p>Warning</p> <p>Don't try to open the imaging camera (TODO serial number) with the ThorCam software else your acquisition might fail.</p>"},{"location":"getting_started/#find-the-eigen-frequency-of-the-membrane","title":"Find the eigen-frequency of the membrane","text":"<p>First check and align the defect of the membrane with the pinhole (more on this here).</p> <p>Then use LabOne to find the eigen-modes of the membrane</p>"},{"location":"getting_started/#acquisition","title":"Acquisition","text":"<p>The acquisition process is detailed here and the file format is documented here.</p>"},{"location":"getting_started/#calibration-data","title":"Calibration data","text":"<ul> <li>Turn off the drive and let the membrane ringdown.</li> <li>Run the calibration cell.</li> </ul>"},{"location":"getting_started/#mode-shape","title":"Mode shape","text":"<ul> <li>Turn on the drive and find back the mode (you shouldn't need to do a sweep).</li> <li>Run the acquisition cell.</li> </ul>"},{"location":"getting_started/#analyze-the-data","title":"Analyze the data","text":"<p>Analyze the data in a separate notebook not to clutter the acquisition notebook. Below is a example of what you can do. For more explanation see here.</p> <pre><code>\"\"\"\nTODO sample analysis script\n\"\"\"\n</code></pre>"},{"location":"membrane_holder/assembly_and_loading/","title":"Assembly of the holder","text":""},{"location":"optical_setup/alignment_FAQ/","title":"Alignment frequently asked questions","text":""},{"location":"optical_setup/alignment_FAQ/#why-is-the-imaging-setup-tilted","title":"Why is the imaging setup tilted","text":"<p>We tilted it to get rid of reflections on the vacuum chamber glass.</p> <p>Note</p> <p>The main beam-splitter in the black box is also tilted. This is because we observed also less parasitic reflections doing so.</p>"},{"location":"optical_setup/alignment_FAQ/#can-i-move-this-lens-or-mirror","title":"Can I move this lens or mirror","text":"<p>Only of you know what you are doing. The hardest part to align are (in decreasing difficulty):</p> <ol> <li> <p>Pinhole + Photo-diode + Pinhole Camera: This part isn't so tricky but     can take a long time to inexperienced hands. Please follow the protocol     here to realign.</p> <p>Warning</p> <p>Moving the lenses on the pinhole alignment might be tricky. We don't now to what extend the cropped beam (by the pinhole) move when doing so and if it remains in the sensitive region of the photo-diode. Anyway, if you feel you lost some signal while doing so, move slightly the photo-diode to get signal back. Don't move the camera and the pinhole in this case, else you are good to follow the protocol here.</p> </li> <li> <p>Laser beam Getting the laser beam hitting the membrane in a nice way can     take time. If you don't see fringes it means the     you are in the situation depicted in the following figure.</p> <p>TODO schema</p> </li> <li> <p>Imaging setup You can move this part of the setup. It's not difficult to align     if you understand how a camera+objective setups works. We encourage you to move the mirror between     the objective and the camera to tune your image.</p> </li> </ol>"},{"location":"optical_setup/alignment_FAQ/#too-much-fringes-on-the-membrane","title":"Too much fringes on the membrane","text":"<p>The membrane isn't parallel to the back-glass slab. Please read the documentation here to correct that.</p>"},{"location":"optical_setup/pinhole_alignment/","title":"Pinhole alignment protocol","text":"<p>TODO schema</p>"},{"location":"optical_setup/pinhole_alignment/#light-protocol","title":"Light protocol","text":"<p>This protocol is quick. You should do it each time you load a new membrane.</p> <ol> <li> <p>Tune the \"pinhole\" mirror to see the defect on the center of the camera. This    should correspond to the middle of the pinhole.</p> </li> <li> <p>Optional Slightly move the photo-diode to get maximum power. If you lose the photo-diode position it can be hard to get back to it.</p> </li> </ol>"},{"location":"optical_setup/pinhole_alignment/#heavy-protocol","title":"Heavy protocol","text":"<p>This protocol is longer to implement but definitely doable. Aligning a cavity is highly likely much harder. Apply this protocol if you're not confident about the location of the pinhole.</p> <ol> <li> <p>Examine the setup</p> </li> <li> <p>Remove the pinhole and move the camera so the sensor lands at the place the pinhole was before.</p> </li> <li> <p>Tune the focus lens and the mirror to center the defect on the camera.</p> </li> <li> <p>move the camera back to give some space for the pinhole</p> </li> <li> <p>Draw a cross in ThorCam software on the defect</p> </li> <li> <p>Put the pinhole on the optical path such that you see on the camera the    bright spot from the pinhole at the exact location of the defect (use the cross drawn at step 5) to help</p> </li> <li> <p>Put back the camera as its original place and center the defect. You can also mark the coordinates of the pinhole on a paper you keep preciously.</p> </li> <li> <p>Put the photo-diode back after the pinhole. It can take a long time to get back some signal since the sensitive region of the photo-diode is small.</p> </li> </ol>"},{"location":"optical_setup/presentation/","title":"Optical setup presentation","text":"<p>TODO Schema</p>"},{"location":"optical_setup/presentation/#the-inteferometric-setup","title":"The inteferometric setup","text":""},{"location":"optical_setup/presentation/#the-pinhole-trick","title":"The pinhole trick","text":"<p>To be able to find the mode we looked</p>"},{"location":"programs/acquisition_tutorial/","title":"Acquisition explanations","text":"<p>TODO</p>"},{"location":"programs/analysis/","title":"Analysis script explanations","text":"<p>TODO</p>"},{"location":"programs/file_format/","title":"File format","text":"<p>The python library use the <code>hdf5</code> file format to store one acquisition (calibration + strobed videos).</p> <p>The hdf5 structure is as follow</p> <pre><code>TODO batter representtion of attributes\n\n\nfile.h5  (2 objects, 6 attributes)\n\u2502   \u251c\u2500\u2500 duty cycle strobe (%)  5\n\u2502   \u251c\u2500\u2500 exposure_time_us  15000\n\u2502   \u251c\u2500\u2500 frame_shape  [1080 1440]\n\u2502   \u251c\u2500\u2500 laser  L785\n\u2502   \u251c\u2500\u2500 membrane  topo\n\u2502   \u2514\u2500\u2500 sensing region  1\n\u251c\u2500\u2500 bias calibration  (3 objects)\n\u2502   \u251c\u2500\u2500 biases  (100,), float64\n\u2502   \u251c\u2500\u2500 photos  (100, 1080, 1440), float64  # Average of the below videos\n\u2502   \u2514\u2500\u2500 videos  (100, 10, 1080, 1440), uint16\n\u2514\u2500\u2500 stroboscopic  (10 objects, 4 attributes)\n    \u251c\u2500\u2500 acquisition time  -284.05384135246277\n    \u251c\u2500\u2500 drive amplitude  0.04999580380099335\n    \u251c\u2500\u2500 drive frequency  1307205.9200001007\n    \u251c\u2500\u2500 strobe detuning  0.0799998992588371\n    \u251c\u2500\u2500 video0  (288, 1080, 1440), uint16\n    \u2502   \u251c\u2500\u2500 bias(V)  -2.7222222222222223\n    \u2502   \u2514\u2500\u2500 fps  20.0\n    \u251c\u2500\u2500 video1  (288, 1080, 1440), uint16\n    \u2502   \u251c\u2500\u2500 bias(V)  -2.166666666666667\n    \u2502   \u2514\u2500\u2500 fps  20.0\n    ...\n</code></pre> <p>You can furthermore explore the file structure with this tool: https://myhdf5.hdfgroup.org/ (It works well even with the 13 gigabytes files the acquisition script produce).</p> <p>To access the raw data, use the <code>h5py</code> library documented here: https://docs.h5py.org/.</p> <p>The analysis tools (to get nice plots of the membrane) are documented in the reference. A more step by step tutorial is provided in analysis script explanations.</p>"},{"location":"programs/python_module_reference/","title":"Reference","text":""},{"location":"programs/python_module_reference/#analysis","title":"Analysis","text":""},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis","title":"<code>StdAnalysis</code>","text":"<p>Standard analysis class.</p> <p>There is a bunch of instance variables that represent the different steps of the analysis. These variables are None by default. As you run the different functions of the class, these variables will get populated.</p> <p>There is a special <code>StdAnalysis.compute_all()</code> function that will perform the whole analysis.</p> <p>If you want to adapt this class to another file format than the default hdf5 format from the original project, you can subclass the class and override all the functions that access to <code>StdAnalysis._file</code> to ada pt to your custom file format.</p> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>class StdAnalysis:\n    \"\"\"\n    Standard analysis class.\n\n    There is a bunch of instance variables that represent the different steps of the analysis.\n    These variables are None by default. As you run the different functions of the class, these variables will get populated.\n\n    There is a special `StdAnalysis.compute_all()` function that will perform the whole analysis.\n\n    If you want to adapt this class to another file format than the default\n    hdf5 format from the original project, you can subclass the class and\n    override all the functions that access to `StdAnalysis._file` to ada pt to\n    your custom file format.\n    \"\"\"\n\n    calibration_biases = None\n    calibration_values = None\n    calibration_slopes = None\n\n    fully_calibrated_images = None\n    phase_corrected_images = None\n    calibration_slopes_video_indexed = None\n\n    mode_image = None\n    \"Mode image not masked with the membrane shapes\"\n\n    best_video_index = None\n    \"indices of video with the bigest interferometer sensitivity for each camera pixel\"\n\n    mask = None\n    \"Membrane shape mask\"\n\n    masked_image = None\n    \"Mode image masked using `self.mask`\"\n\n    def __init__(self, file):\n        \"\"\"\n        Initialise the analysis class.\n\n        Args:\n            file (h5py.File): A hdf5 file handle (for instance\n        \"\"\"\n        self._file = file\n\n    @property\n    def is_open(self):\n        return self._file\n\n    def file_open_or_fail(self):\n        \"\"\"\n        Check if file is open or fail with `IOError`\n        \"\"\"\n        if not self.is_open:\n            raise IOError(\"HDF5 file closed too early\")\n\n    def get_calibration_photos(self):\n        self.file_open_or_fail()\n        return self._file[\"bias calibration\"][\"photos\"][...]\n\n    def get_calibration_biases(self):\n        self.file_open_or_fail()\n        return self._file[\"bias calibration\"][\"biases\"][...]\n\n    def get_videos(self):\n        \"\"\"\n        Return a tuple of two generators:\n            - One yielding the videos\n            - One yielding the biases\n\n        These generators can't outlive the file handle\n        \"\"\"\n        self.file_open_or_fail()\n        return (\n            (  # Use a generator to load lazily the videos\n                self._file[\"stroboscopic\"][k][...]\n                for k in self._file[\"stroboscopic\"].keys()\n            ),\n            [\n                self._file[\"stroboscopic\"][k].attrs[\"bias(V)\"]\n                for k in self._file[\"stroboscopic\"].keys()\n            ],\n            len(self._file[\"stroboscopic\"].keys()),\n        )\n\n    def smooth_calibration(self, window=np.array([0.1, 0.25, 0.3, 0.25, 0.1])):\n        \"\"\"\n        Smooth the calibration data points.\n\n        Sets `self.calibration_biases` and `self.calibration_values` to\n        respectively the biases values and the smoothed pixel intensity values.\n\n        Args:\n            window (np.ndarray): The kernel for the smoothing\n\n\n        \"\"\"\n        if len(window.shape) &gt; 1:\n            raise ValueError(\"Smoothing kernel should be 1-dimensional\")\n\n        photos = self.get_calibration_photos()\n        biases = self.get_calibration_biases()\n        smoothed = np.apply_along_axis(\n            lambda x: np.convolve(window, x, mode=\"valid\"), axis=0, arr=photos\n        )\n        offset = window.size // 2  # number of values missing on each side\n\n        self.calibration_biases = biases[offset:-offset]\n        self.calibration_values = smoothed\n\n    def compute_calibration_slopes(self):\n        \"\"\"\n        Sets `self.calibration_slopes` to the slopes of calibration curves\n        \"\"\"\n        if self.calibration_values is None:\n            raise Exception(\"\")\n        self.calibration_slopes = np.array(\n            np.gradient(self.calibration_values, axis=0)\n        )  # TODO: Provide X values for the gradient\n\n    @staticmethod\n    def std_image(video):\n        \"\"\"\n        Compute the mode shape (with phase) image for a single video.\n\n        It performs the following actions:\n            - Compute the stddev of each pixel along time\n            - Look at pixel wih the largest stddev. It is taken as a reference\n            - For each pixel time trace we compute the dot product along time\n              with the reference pixel time trace. This dot product inform use\n              on the phase of this point of the membrane with respect to the\n              reference pixel\n                - If the dot product is negative we put `- std(pixel time trace)` on the image\n                - Else we put `std(pixel time trace)` on the image\n\n        Args:\n            video (np.ndarray): The raw video from the camera.\n        Returns:\n            np.ndarray: `\u00b1 std(video, axis=time)`. The \u00b1 is determined according to the phase with respect to the reference pixel time trace\n        \"\"\"\n        std = np.std(video, axis=0)\n\n        # Get coordinates of the brightest pixel\n        bightest_pixel: tuple[int, int] = np.unravel_index(\n            np.argmax(std, axis=None), std.shape\n        )  # pyright: ignore\n\n        transposed_normalized_video = np.transpose(\n            video - np.mean(video, axis=0), (1, 2, 0)\n        )\n\n        brightest_pixel_time_trace = transposed_normalized_video[\n            bightest_pixel[0], bightest_pixel[1], :\n        ]\n\n        video_dot: np.ndarray = np.dot(\n            transposed_normalized_video, brightest_pixel_time_trace\n        )\n        return np.where(video_dot &gt; 0, std, -std)\n\n    ## Function that takes in an HDF5 file and returns a list of calibrated videos.\n    def compute_independant_video_images(self):\n        \"\"\"\n        Apply `self.std_images` to each video and then apply a proportional\n        correction using the calibration data to correct\n          1. the spatial intensity variation of the laser\n          2. the position on the interference fringe (if light intensity\n             increase or decrease when the membrane displacement increase).\n\n        The result is stored in `self.fully_calibrated_images`\n\n        We also store the image apply only the second correction to `self.phase_corrected_images`\n        \"\"\"\n\n        if self.calibration_slopes is None:\n            raise Exception(\"\")\n\n        videos, specific_biases, video_number = self.get_videos()\n\n        def find_nearest(array, value):\n            idx = np.searchsorted(array, value, side=\"left\")\n            if idx &gt; 0 and (\n                idx == len(array)\n                or np.abs(value - array[idx - 1]) &lt; np.abs(value - array[idx])\n            ):\n                return idx - 1\n            return idx\n\n        # Find the indices in calibration_* arrays corresponding to the videos\n        specific_biases_indices = np.array(\n            [\n                find_nearest(self.calibration_biases, specific_bias)\n                for specific_bias in specific_biases\n            ]\n        )\n        specific_biases_calibration_slope: np.ndarray = (\n            1 / self.calibration_slopes[specific_biases_indices, :, :]\n        )\n\n        std_images = np.array(\n            [\n                self.std_image(video)\n                for video in tqdm(\n                    videos,\n                    total=video_number,\n                    desc=\"Processing videos\",\n                )\n            ]\n        )\n        print(\"Applying calibration sign...\", end=\"\")\n        std_images_phase_calibrated = std_images * np.where(\n            specific_biases_calibration_slope &gt; 0, 1, -1\n        )\n        std_images_full_calibrated = std_images * specific_biases_calibration_slope\n        print(\"Done\")\n        self.fully_calibrated_images = std_images_full_calibrated  # Phase+amplitude\n        self.phase_corrected_images = std_images_phase_calibrated  # phase only\n        self.calibration_slopes_video_indexed = self.calibration_slopes[\n            specific_biases_indices\n        ]\n\n    def combine_images(self):\n        \"\"\"\n        Combine images.\n\n            1. Flip the phase of some images so that all the images show the same global phase\n            2. For each pixel take the value from the most sensitive video shot (according to calibration data)\n        \"\"\"\n        if self.fully_calibrated_images is None:\n            raise Exception(\"\")\n        if self.phase_corrected_images is None:\n            raise Exception(\"\")\n        if self.calibration_slopes_video_indexed is None:\n            raise Exception(\"\")\n        absolute_slopes = np.abs(self.calibration_slopes_video_indexed)\n        to_flip = np.array(\n            [\n                np.vdot(\n                    self.phase_corrected_images[0, :, :],\n                    self.phase_corrected_images[i, :, :],\n                )\n                for i in trange(0, self.phase_corrected_images.shape[0])\n            ]\n        )\n        corrected_photos: np.ndarray = np.where(\n            to_flip[:, None, None] &gt; 0,\n            self.fully_calibrated_images,\n            -self.fully_calibrated_images,  # pyright: ignore\n        )\n\n        # Initialize empty images\n        self.mode_image = np.empty(corrected_photos.shape[1:])\n        self.best_video_index = np.empty(corrected_photos.shape[1:], dtype=np.uint64)\n        # For each pixel get the best video time trace\n        with np.nditer(\n            self.mode_image,\n            flags=[\"multi_index\"],\n            op_flags=[\"writeonly\"],  # pyright: ignore\n        ) as it:\n            for pixel in tqdm(it, total=self.mode_image.size):\n                best_video = np.argmax(\n                    absolute_slopes[:, it.multi_index[0], it.multi_index[1]]\n                )\n                pixel[...] = corrected_photos[  # pyright: ignore\n                    best_video, it.multi_index[0], it.multi_index[1]\n                ]\n                self.best_video_index[it.multi_index[0], it.multi_index[1]] = best_video\n\n    def apply_membrane_shape_masking(self, threshold=1.0, sigma=100):\n        \"\"\"\n        Compute the membrane shape in `self.mask`.\n\n        The term sensitivity refers to the amplitude of the calibration data curve.\n\n        Algorithm:\n            - For each pixel:\n                1. Compute the average sensitivity in the nearby area (typical size of `sigma` pixels)\n                2. Pick this pixel as a pixel from the membrane if the pixel sensiti\n        \"\"\"\n        if self.calibration_values is None:\n            raise Exception(\"\")\n        sensitivity = np.std(self.calibration_values, axis=0)\n        smoothed = scipy.ndimage.gaussian_filter(sensitivity, sigma=sigma)\n        self.mask = sensitivity &gt; (threshold * smoothed)\n        self.masked_image = self.mode_image * self.mask\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.best_video_index","title":"<code>best_video_index = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>indices of video with the bigest interferometer sensitivity for each camera pixel</p>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.mask","title":"<code>mask = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Membrane shape mask</p>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.masked_image","title":"<code>masked_image = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mode image masked using <code>self.mask</code></p>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.mode_image","title":"<code>mode_image = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mode image not masked with the membrane shapes</p>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.__init__","title":"<code>__init__(file)</code>","text":"<p>Initialise the analysis class.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>File</code> <p>A hdf5 file handle (for instance</p> required Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def __init__(self, file):\n    \"\"\"\n    Initialise the analysis class.\n\n    Args:\n        file (h5py.File): A hdf5 file handle (for instance\n    \"\"\"\n    self._file = file\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.apply_membrane_shape_masking","title":"<code>apply_membrane_shape_masking(threshold=1.0, sigma=100)</code>","text":"<p>Compute the membrane shape in <code>self.mask</code>.</p> <p>The term sensitivity refers to the amplitude of the calibration data curve.</p> Algorithm <ul> <li>For each pixel:<ol> <li>Compute the average sensitivity in the nearby area (typical size of <code>sigma</code> pixels)</li> <li>Pick this pixel as a pixel from the membrane if the pixel sensiti</li> </ol> </li> </ul> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def apply_membrane_shape_masking(self, threshold=1.0, sigma=100):\n    \"\"\"\n    Compute the membrane shape in `self.mask`.\n\n    The term sensitivity refers to the amplitude of the calibration data curve.\n\n    Algorithm:\n        - For each pixel:\n            1. Compute the average sensitivity in the nearby area (typical size of `sigma` pixels)\n            2. Pick this pixel as a pixel from the membrane if the pixel sensiti\n    \"\"\"\n    if self.calibration_values is None:\n        raise Exception(\"\")\n    sensitivity = np.std(self.calibration_values, axis=0)\n    smoothed = scipy.ndimage.gaussian_filter(sensitivity, sigma=sigma)\n    self.mask = sensitivity &gt; (threshold * smoothed)\n    self.masked_image = self.mode_image * self.mask\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.combine_images","title":"<code>combine_images()</code>","text":"<p>Combine images.</p> <pre><code>1. Flip the phase of some images so that all the images show the same global phase\n2. For each pixel take the value from the most sensitive video shot (according to calibration data)\n</code></pre> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def combine_images(self):\n    \"\"\"\n    Combine images.\n\n        1. Flip the phase of some images so that all the images show the same global phase\n        2. For each pixel take the value from the most sensitive video shot (according to calibration data)\n    \"\"\"\n    if self.fully_calibrated_images is None:\n        raise Exception(\"\")\n    if self.phase_corrected_images is None:\n        raise Exception(\"\")\n    if self.calibration_slopes_video_indexed is None:\n        raise Exception(\"\")\n    absolute_slopes = np.abs(self.calibration_slopes_video_indexed)\n    to_flip = np.array(\n        [\n            np.vdot(\n                self.phase_corrected_images[0, :, :],\n                self.phase_corrected_images[i, :, :],\n            )\n            for i in trange(0, self.phase_corrected_images.shape[0])\n        ]\n    )\n    corrected_photos: np.ndarray = np.where(\n        to_flip[:, None, None] &gt; 0,\n        self.fully_calibrated_images,\n        -self.fully_calibrated_images,  # pyright: ignore\n    )\n\n    # Initialize empty images\n    self.mode_image = np.empty(corrected_photos.shape[1:])\n    self.best_video_index = np.empty(corrected_photos.shape[1:], dtype=np.uint64)\n    # For each pixel get the best video time trace\n    with np.nditer(\n        self.mode_image,\n        flags=[\"multi_index\"],\n        op_flags=[\"writeonly\"],  # pyright: ignore\n    ) as it:\n        for pixel in tqdm(it, total=self.mode_image.size):\n            best_video = np.argmax(\n                absolute_slopes[:, it.multi_index[0], it.multi_index[1]]\n            )\n            pixel[...] = corrected_photos[  # pyright: ignore\n                best_video, it.multi_index[0], it.multi_index[1]\n            ]\n            self.best_video_index[it.multi_index[0], it.multi_index[1]] = best_video\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.compute_calibration_slopes","title":"<code>compute_calibration_slopes()</code>","text":"<p>Sets <code>self.calibration_slopes</code> to the slopes of calibration curves</p> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def compute_calibration_slopes(self):\n    \"\"\"\n    Sets `self.calibration_slopes` to the slopes of calibration curves\n    \"\"\"\n    if self.calibration_values is None:\n        raise Exception(\"\")\n    self.calibration_slopes = np.array(\n        np.gradient(self.calibration_values, axis=0)\n    )  # TODO: Provide X values for the gradient\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.compute_independant_video_images","title":"<code>compute_independant_video_images()</code>","text":"<p>Apply <code>self.std_images</code> to each video and then apply a proportional correction using the calibration data to correct   1. the spatial intensity variation of the laser   2. the position on the interference fringe (if light intensity      increase or decrease when the membrane displacement increase).</p> <p>The result is stored in <code>self.fully_calibrated_images</code></p> <p>We also store the image apply only the second correction to <code>self.phase_corrected_images</code></p> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def compute_independant_video_images(self):\n    \"\"\"\n    Apply `self.std_images` to each video and then apply a proportional\n    correction using the calibration data to correct\n      1. the spatial intensity variation of the laser\n      2. the position on the interference fringe (if light intensity\n         increase or decrease when the membrane displacement increase).\n\n    The result is stored in `self.fully_calibrated_images`\n\n    We also store the image apply only the second correction to `self.phase_corrected_images`\n    \"\"\"\n\n    if self.calibration_slopes is None:\n        raise Exception(\"\")\n\n    videos, specific_biases, video_number = self.get_videos()\n\n    def find_nearest(array, value):\n        idx = np.searchsorted(array, value, side=\"left\")\n        if idx &gt; 0 and (\n            idx == len(array)\n            or np.abs(value - array[idx - 1]) &lt; np.abs(value - array[idx])\n        ):\n            return idx - 1\n        return idx\n\n    # Find the indices in calibration_* arrays corresponding to the videos\n    specific_biases_indices = np.array(\n        [\n            find_nearest(self.calibration_biases, specific_bias)\n            for specific_bias in specific_biases\n        ]\n    )\n    specific_biases_calibration_slope: np.ndarray = (\n        1 / self.calibration_slopes[specific_biases_indices, :, :]\n    )\n\n    std_images = np.array(\n        [\n            self.std_image(video)\n            for video in tqdm(\n                videos,\n                total=video_number,\n                desc=\"Processing videos\",\n            )\n        ]\n    )\n    print(\"Applying calibration sign...\", end=\"\")\n    std_images_phase_calibrated = std_images * np.where(\n        specific_biases_calibration_slope &gt; 0, 1, -1\n    )\n    std_images_full_calibrated = std_images * specific_biases_calibration_slope\n    print(\"Done\")\n    self.fully_calibrated_images = std_images_full_calibrated  # Phase+amplitude\n    self.phase_corrected_images = std_images_phase_calibrated  # phase only\n    self.calibration_slopes_video_indexed = self.calibration_slopes[\n        specific_biases_indices\n    ]\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.file_open_or_fail","title":"<code>file_open_or_fail()</code>","text":"<p>Check if file is open or fail with <code>IOError</code></p> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def file_open_or_fail(self):\n    \"\"\"\n    Check if file is open or fail with `IOError`\n    \"\"\"\n    if not self.is_open:\n        raise IOError(\"HDF5 file closed too early\")\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.get_videos","title":"<code>get_videos()</code>","text":"Return a tuple of two generators <ul> <li>One yielding the videos</li> <li>One yielding the biases</li> </ul> <p>These generators can't outlive the file handle</p> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def get_videos(self):\n    \"\"\"\n    Return a tuple of two generators:\n        - One yielding the videos\n        - One yielding the biases\n\n    These generators can't outlive the file handle\n    \"\"\"\n    self.file_open_or_fail()\n    return (\n        (  # Use a generator to load lazily the videos\n            self._file[\"stroboscopic\"][k][...]\n            for k in self._file[\"stroboscopic\"].keys()\n        ),\n        [\n            self._file[\"stroboscopic\"][k].attrs[\"bias(V)\"]\n            for k in self._file[\"stroboscopic\"].keys()\n        ],\n        len(self._file[\"stroboscopic\"].keys()),\n    )\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.smooth_calibration","title":"<code>smooth_calibration(window=np.array([0.1, 0.25, 0.3, 0.25, 0.1]))</code>","text":"<p>Smooth the calibration data points.</p> <p>Sets <code>self.calibration_biases</code> and <code>self.calibration_values</code> to respectively the biases values and the smoothed pixel intensity values.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>ndarray</code> <p>The kernel for the smoothing</p> <code>array([0.1, 0.25, 0.3, 0.25, 0.1])</code> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>def smooth_calibration(self, window=np.array([0.1, 0.25, 0.3, 0.25, 0.1])):\n    \"\"\"\n    Smooth the calibration data points.\n\n    Sets `self.calibration_biases` and `self.calibration_values` to\n    respectively the biases values and the smoothed pixel intensity values.\n\n    Args:\n        window (np.ndarray): The kernel for the smoothing\n\n\n    \"\"\"\n    if len(window.shape) &gt; 1:\n        raise ValueError(\"Smoothing kernel should be 1-dimensional\")\n\n    photos = self.get_calibration_photos()\n    biases = self.get_calibration_biases()\n    smoothed = np.apply_along_axis(\n        lambda x: np.convolve(window, x, mode=\"valid\"), axis=0, arr=photos\n    )\n    offset = window.size // 2  # number of values missing on each side\n\n    self.calibration_biases = biases[offset:-offset]\n    self.calibration_values = smoothed\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.analysis.StdAnalysis.std_image","title":"<code>std_image(video)</code>  <code>staticmethod</code>","text":"<p>Compute the mode shape (with phase) image for a single video.</p> It performs the following actions <ul> <li>Compute the stddev of each pixel along time</li> <li>Look at pixel wih the largest stddev. It is taken as a reference</li> <li>For each pixel time trace we compute the dot product along time   with the reference pixel time trace. This dot product inform use   on the phase of this point of the membrane with respect to the   reference pixel<ul> <li>If the dot product is negative we put <code>- std(pixel time trace)</code> on the image</li> <li>Else we put <code>std(pixel time trace)</code> on the image</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>ndarray</code> <p>The raw video from the camera.</p> required <p>Returns:     np.ndarray: <code>\u00b1 std(video, axis=time)</code>. The \u00b1 is determined according to the phase with respect to the reference pixel time trace</p> Source code in <code>strobing_interferometer/analysis.py</code> <pre><code>@staticmethod\ndef std_image(video):\n    \"\"\"\n    Compute the mode shape (with phase) image for a single video.\n\n    It performs the following actions:\n        - Compute the stddev of each pixel along time\n        - Look at pixel wih the largest stddev. It is taken as a reference\n        - For each pixel time trace we compute the dot product along time\n          with the reference pixel time trace. This dot product inform use\n          on the phase of this point of the membrane with respect to the\n          reference pixel\n            - If the dot product is negative we put `- std(pixel time trace)` on the image\n            - Else we put `std(pixel time trace)` on the image\n\n    Args:\n        video (np.ndarray): The raw video from the camera.\n    Returns:\n        np.ndarray: `\u00b1 std(video, axis=time)`. The \u00b1 is determined according to the phase with respect to the reference pixel time trace\n    \"\"\"\n    std = np.std(video, axis=0)\n\n    # Get coordinates of the brightest pixel\n    bightest_pixel: tuple[int, int] = np.unravel_index(\n        np.argmax(std, axis=None), std.shape\n    )  # pyright: ignore\n\n    transposed_normalized_video = np.transpose(\n        video - np.mean(video, axis=0), (1, 2, 0)\n    )\n\n    brightest_pixel_time_trace = transposed_normalized_video[\n        bightest_pixel[0], bightest_pixel[1], :\n    ]\n\n    video_dot: np.ndarray = np.dot(\n        transposed_normalized_video, brightest_pixel_time_trace\n    )\n    return np.where(video_dot &gt; 0, std, -std)\n</code></pre>"},{"location":"programs/python_module_reference/#acquisition","title":"Acquisition","text":""},{"location":"programs/python_module_reference/#strobing_interferometer.acquisition.Acquisition","title":"<code>Acquisition</code>","text":"Source code in <code>strobing_interferometer/acquisition.py</code> <pre><code>class Acquisition:\n    path = None\n\n    def __init__(\n        self,\n        path: Union[Path, str],\n        exposure_time_us: int,\n        n_calib: int,\n        bias_range: Tuple[float, float],\n        vid_len: int = 288,\n        strobe_detuning: float = 0.5,\n        instruments_manager=None,\n        **kwargs,\n    ):\n        self.path = Path(path)\n        if self.path.is_dir():\n            self.path = self.path\n        if self.path.exists():\n            raise ValueError(\n                \"The target file for the acquisition already exists ({})\".format(\n                    str(self.path)\n                )\n            )\n\n        self.acquisition_kwargs = kwargs\n\n        self.strobe_detuning = strobe_detuning\n        self.vid_len = vid_len\n        self.exposure_time_us = exposure_time_us\n        self.n_calib = n_calib\n        if bias_range[0] &gt;= bias_range[1]:\n            raise ValueError(\"Please provide a valid bias range\")\n        self.bias_range = bias_range\n        if instruments_manager is None:\n            self.instruments_manager = InstrumentManager.get_default()\n        else:\n            self.instruments_manager = instruments_manager\n\n        self.kwargs = kwargs\n\n    def acquire_calibration(self):\n        \"\"\"\n        Record bias calibration. This will use several gigas of RAM.\n\n        Should not be used while the thorcam software is open\n        \"\"\"\n\n        self.biases = np.linspace(self.bias_range[0], self.bias_range[1], 100)\n\n        biases = self.biases\n\n        self.instruments_manager.strobe_at(detun=2000)\n\n        self.instruments_manager.drive_off()\n\n        self.instruments_manager.goToBias(biases[0], speed=1.0)\n\n        time.sleep(1)\n\n        with TLCameraSDK() as sdk:  # TODO: move the camera handling logic to the instument_manager\n            available_cameras = sdk.discover_available_cameras()\n            if len(available_cameras) &lt; 1:\n                raise Exception(\"no cameras detected\")\n\n            with sdk.open_camera(self.instruments_manager.camera_sn) as camera:\n                camera.image_poll_timeout_ms = 100\n                camera.exposure_time_us = self.exposure_time_us\n\n                camera.frames_per_trigger_zero_for_unlimited = 1\n                frame_shape = (\n                    camera.sensor_height_pixels,\n                    camera.sensor_width_pixels,\n                )\n\n                # exposure time sanity check\n                print(\"Exposure time sanity check...\", end=\"\")\n                camera.arm(2)\n                camera.issue_software_trigger()\n                frame = None\n                t0 = time.time()\n                while frame is None:\n                    if time.time() - t0 &gt; 2:\n                        print(\n                            \"The frame I'm waiting may have been dropped. Do not hesitate to stop the script if you think it is the case\"\n                        )\n                    frame = camera.get_pending_frame_or_null()\n\n                print(\"Done\")\n                n_saturating = np.sum(np.array(frame.image_buffer) &gt; 1020)\n                frame_max = np.max(frame.image_buffer)\n                print(\"Number of saturating pixels:\", n_saturating)\n                if (\n                    n_saturating &gt; 50000\n                ):  # arbitrary (= few percent of the image are saturating)\n                    raise Exception(\"Saturating image.please decrease exposure time\")\n                if frame_max &lt; 1000:\n                    raise Exception(\"Too dim image.please increase exposure time\")\n                camera.disarm()\n\n                print(\"Acquiring calibration data\")\n                buffer = np.empty(\n                    (len(biases), self.n_calib, *frame_shape), dtype=np.uint16\n                )\n                with h5py.File(self.path, \"a\") as f:\n                    camera.frames_per_trigger_zero_for_unlimited = self.n_calib\n                    camera.arm(2)\n                    for i, bias in tqdm(enumerate(biases), total=len(biases)):\n                        self.instruments_manager.goToBias(bias)\n                        time.sleep(0.05)\n                        camera.issue_software_trigger()\n                        prev_frame = None\n                        for j in range(camera.frames_per_trigger_zero_for_unlimited):\n                            frame = None\n                            while frame is None:\n                                frame = camera.get_pending_frame_or_null()\n                            buffer[i, j] = frame.image_buffer\n                            if (\n                                prev_frame is not None\n                                and frame.frame_count - prev_frame &gt; 1\n                            ):\n                                raise Exception(\n                                    \"Dropped frame at bias n\u00b0{i} (Bias={bias})\"\n                                )\n                            prev_frame = frame.frame_count\n                    camera.disarm()\n                    f.attrs[\"frame_shape\"] = np.array(frame_shape)\n                    f.attrs.update(self.kwargs)\n                    grp = f.create_group(\"bias calibration\")\n                    grp.create_dataset(\"photos\", data=np.mean(buffer, axis=1))\n                    grp.create_dataset(\"videos\", data=buffer, dtype=np.uint16)\n                    grp.create_dataset(\"biases\", data=biases)\n\n        print(\"Please turn on the drive and find the right frequency\")\n\n    def acquire_modeshape(self):\n        freq = self.instruments_manager.get_drive_freq()\n\n        biases_vid = np.array([self.biases[10 * i + 5] for i in range(10)])\n\n        self.instruments_manager.strobe_on()\n        self.instruments_manager.strobe_at(detun=0.5)\n\n        time.sleep(1)\n\n        begin_time = time.time()\n\n        strobe_attrs = {\n            \"strobe detuning\": self.instruments_manager.get_strobe_frequency() - freq,\n            \"drive amplitude\": self.instruments_manager.get_drive_amplitude(),\n            \"drive frequency\": freq,\n        }\n\n        self.instruments_manager.goToBias(biases_vid[0], speed=1)\n\n        print(f\"Saving to `{self.path}`\")\n        with TLCameraSDK() as sdk:\n            available_cameras = sdk.discover_available_cameras()\n            if len(available_cameras) &lt; 1:\n                print(\"no cameras detected\")\n            else:\n                with sdk.open_camera(self.instruments_manager.camera_sn) as camera:\n\n                    camera.image_poll_timeout_ms = 1000\n                    camera.exposure_time_us = self.exposure_time_us\n\n                    camera.frame_rate_control_value = 20\n\n                    cam_shape = (\n                        self.vid_len,\n                        camera.sensor_height_pixels,\n                        camera.sensor_width_pixels,\n                    )\n\n                    buffer = np.empty((len(biases_vid), *cam_shape), dtype=np.uint16)\n\n                    with h5py.File(self.path, \"a\") as f:\n                        if \"stroboscopic\" in f:\n                            del f[\"stroboscopic\"]\n                        grp = f.create_group(\"stroboscopic\")\n                        grp.attrs.update(strobe_attrs)\n                        buffer = np.empty(cam_shape)\n                        n_video = len(biases_vid)\n                        for i, bias in enumerate(biases_vid):\n                            print(\"Going to right bias...\", end=\"\")\n                            self.instruments_manager.goToBias(bias)\n                            print(\" Sleeping...\", end=\"\")\n                            time.sleep(3)  # TODO: make tunable\n                            print(\" Arming camera...\")\n                            camera.frames_per_trigger_zero_for_unlimited = self.vid_len\n                            # This buffer size comes from thorlabs' live camera example. Let's keep it\n                            camera.arm(2)\n                            camera.issue_software_trigger()\n                            prev_frame = None\n                            for j in trange(\n                                self.vid_len, desc=f\"Video n\u00b0{i}/{n_video}\"\n                            ):\n                                frame = None\n                                while frame is None:\n                                    frame = camera.get_pending_frame_or_null()\n                                buffer[j] = frame.image_buffer\n                                if (\n                                    prev_frame is not None\n                                    and frame.frame_count - prev_frame &gt; 1\n                                ):\n                                    raise Exception(\n                                        f\"Dropped frame at bias n\u00b0{i} (Bias={bias})\"\n                                    )\n                                prev_frame = frame.frame_count\n                            fps = camera.get_measured_frame_rate_fps()  # TODO: fix\n                            print(\"Disarming... \", end=\"\")\n                            camera.disarm()\n                            print(\"Saving...\")\n                            dset = grp.create_dataset(\n                                f\"video{i}\", data=buffer, dtype=np.uint16\n                            )\n                            dset.attrs[\"fps\"] = fps\n                            dset.attrs[\"bias(V)\"] = bias\n                            f.flush()\n                        grp.attrs[\"acquisition time\"] = begin_time - time.time()\n        print(\"Data acquisition is succesfully completed.\")\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.acquisition.Acquisition.acquire_calibration","title":"<code>acquire_calibration()</code>","text":"<p>Record bias calibration. This will use several gigas of RAM.</p> <p>Should not be used while the thorcam software is open</p> Source code in <code>strobing_interferometer/acquisition.py</code> <pre><code>def acquire_calibration(self):\n    \"\"\"\n    Record bias calibration. This will use several gigas of RAM.\n\n    Should not be used while the thorcam software is open\n    \"\"\"\n\n    self.biases = np.linspace(self.bias_range[0], self.bias_range[1], 100)\n\n    biases = self.biases\n\n    self.instruments_manager.strobe_at(detun=2000)\n\n    self.instruments_manager.drive_off()\n\n    self.instruments_manager.goToBias(biases[0], speed=1.0)\n\n    time.sleep(1)\n\n    with TLCameraSDK() as sdk:  # TODO: move the camera handling logic to the instument_manager\n        available_cameras = sdk.discover_available_cameras()\n        if len(available_cameras) &lt; 1:\n            raise Exception(\"no cameras detected\")\n\n        with sdk.open_camera(self.instruments_manager.camera_sn) as camera:\n            camera.image_poll_timeout_ms = 100\n            camera.exposure_time_us = self.exposure_time_us\n\n            camera.frames_per_trigger_zero_for_unlimited = 1\n            frame_shape = (\n                camera.sensor_height_pixels,\n                camera.sensor_width_pixels,\n            )\n\n            # exposure time sanity check\n            print(\"Exposure time sanity check...\", end=\"\")\n            camera.arm(2)\n            camera.issue_software_trigger()\n            frame = None\n            t0 = time.time()\n            while frame is None:\n                if time.time() - t0 &gt; 2:\n                    print(\n                        \"The frame I'm waiting may have been dropped. Do not hesitate to stop the script if you think it is the case\"\n                    )\n                frame = camera.get_pending_frame_or_null()\n\n            print(\"Done\")\n            n_saturating = np.sum(np.array(frame.image_buffer) &gt; 1020)\n            frame_max = np.max(frame.image_buffer)\n            print(\"Number of saturating pixels:\", n_saturating)\n            if (\n                n_saturating &gt; 50000\n            ):  # arbitrary (= few percent of the image are saturating)\n                raise Exception(\"Saturating image.please decrease exposure time\")\n            if frame_max &lt; 1000:\n                raise Exception(\"Too dim image.please increase exposure time\")\n            camera.disarm()\n\n            print(\"Acquiring calibration data\")\n            buffer = np.empty(\n                (len(biases), self.n_calib, *frame_shape), dtype=np.uint16\n            )\n            with h5py.File(self.path, \"a\") as f:\n                camera.frames_per_trigger_zero_for_unlimited = self.n_calib\n                camera.arm(2)\n                for i, bias in tqdm(enumerate(biases), total=len(biases)):\n                    self.instruments_manager.goToBias(bias)\n                    time.sleep(0.05)\n                    camera.issue_software_trigger()\n                    prev_frame = None\n                    for j in range(camera.frames_per_trigger_zero_for_unlimited):\n                        frame = None\n                        while frame is None:\n                            frame = camera.get_pending_frame_or_null()\n                        buffer[i, j] = frame.image_buffer\n                        if (\n                            prev_frame is not None\n                            and frame.frame_count - prev_frame &gt; 1\n                        ):\n                            raise Exception(\n                                \"Dropped frame at bias n\u00b0{i} (Bias={bias})\"\n                            )\n                        prev_frame = frame.frame_count\n                camera.disarm()\n                f.attrs[\"frame_shape\"] = np.array(frame_shape)\n                f.attrs.update(self.kwargs)\n                grp = f.create_group(\"bias calibration\")\n                grp.create_dataset(\"photos\", data=np.mean(buffer, axis=1))\n                grp.create_dataset(\"videos\", data=buffer, dtype=np.uint16)\n                grp.create_dataset(\"biases\", data=biases)\n\n    print(\"Please turn on the drive and find the right frequency\")\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.acquisition.InstrumentManager","title":"<code>InstrumentManager</code>","text":"<p>Class to manage the instruments</p> Source code in <code>strobing_interferometer/acquisition.py</code> <pre><code>class InstrumentManager:\n    \"\"\"\n    Class to manage the instruments\n    \"\"\"\n\n    default_manager = None\n    rigol_addr = \"TCPIP0::10.209.65.139::inst0::INSTR\"\n    rigol_channel = 1\n    bias_gain = 0.1\n\n    hf2_serial = \"dev1224\"\n    camera_sn = \"25779\"\n\n    @classmethod\n    def get_default(cls):\n        if cls.default_manager is not None:\n            return cls.default_manager\n        cls.default_manager = cls()\n        return cls.default_manager\n\n    # TODO: camera_lock = None\n\n    def __init__(self):\n        self.rigol = DG1032Z(self.rigol_addr)\n        self.rigol.channel = self.rigol_channel\n        self.hf2 = HF2(self.hf2_serial, 1)\n\n    def get_drive_freq(self):\n        return self.hf2.daq.getDouble(\"/dev1224/oscs/0/freq\")\n\n    def get_strobe_frequency(self):\n        return self.hf2.daq.getDouble(\"/dev1224/sigouts/0/amplitudes/6\")\n\n    def get_drive_amplitude(self):\n        return self.rigol.frequency\n\n    def set_freqs(self, x, detun=1):\n        \"\"\"\n        Function to set the drive frequency and the detuning of the strobing\n\n        This function doesn't turn on the strobing.\n\n        Args:\n            freq (float): The drive frequency\n            detun (float): The detuning of the strobe\n        \"\"\"\n        self.hf2.daq.setDouble(\"/dev1224/oscs/0/freq\", x)\n        self.rigol.frequency = x + detun\n\n    def strobe_at(self, detun: float = 1.0):\n        \"\"\"\n        Function that retrieves the drive frequency and set the strobe frequency\n\n        This function doesn't turn on the strobing.\n\n        Args:\n            detun (float): The strobe detuning (positive means strobing at higher frequency)\n        \"\"\"\n        f = self.hf2.daq.getDouble(\"/dev1224/oscs/0/freq\")\n        self.rigol.frequency = f + detun\n\n    def strobe_on(self):\n        self.rigol.output = True\n\n    def strobe_off(self):\n        self.rigol.output = False\n\n    def drive_on(self):\n        self.hf2.daq.setInt(\"/dev1224/sigouts/0/enables/6\", 1)\n\n    def drive_off(self):\n        self.hf2.daq.setInt(\"/dev1224/sigouts/0/enables/6\", 0)\n\n    def goToBias(self, new_bias, speed=0.2, step_size=0.005):\n        \"\"\"\n        Function to go smoothly from one bias value to another\n\n        speed in 10V/s\n        step in 10V\n        \"\"\"\n        delta_t = step_size / speed\n        old_bias = self.hf2.daq.getDouble(\"/dev1224/sigouts/1/offset\") / self.bias_gain\n        span = abs(new_bias - old_bias)\n        n_step = int(np.ceil(span / step_size))  # pyright: ignore  # pyright is drunk\n        steps = np.linspace(old_bias, new_bias, n_step)\n        for b in steps:\n            self.hf2.daq.setDouble(\"/dev1224/sigouts/1/offset\", b * self.bias_gain)\n            time.sleep(delta_t)\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.acquisition.InstrumentManager.goToBias","title":"<code>goToBias(new_bias, speed=0.2, step_size=0.005)</code>","text":"<p>Function to go smoothly from one bias value to another</p> <p>speed in 10V/s step in 10V</p> Source code in <code>strobing_interferometer/acquisition.py</code> <pre><code>def goToBias(self, new_bias, speed=0.2, step_size=0.005):\n    \"\"\"\n    Function to go smoothly from one bias value to another\n\n    speed in 10V/s\n    step in 10V\n    \"\"\"\n    delta_t = step_size / speed\n    old_bias = self.hf2.daq.getDouble(\"/dev1224/sigouts/1/offset\") / self.bias_gain\n    span = abs(new_bias - old_bias)\n    n_step = int(np.ceil(span / step_size))  # pyright: ignore  # pyright is drunk\n    steps = np.linspace(old_bias, new_bias, n_step)\n    for b in steps:\n        self.hf2.daq.setDouble(\"/dev1224/sigouts/1/offset\", b * self.bias_gain)\n        time.sleep(delta_t)\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.acquisition.InstrumentManager.set_freqs","title":"<code>set_freqs(x, detun=1)</code>","text":"<p>Function to set the drive frequency and the detuning of the strobing</p> <p>This function doesn't turn on the strobing.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>float</code> <p>The drive frequency</p> required <code>detun</code> <code>float</code> <p>The detuning of the strobe</p> <code>1</code> Source code in <code>strobing_interferometer/acquisition.py</code> <pre><code>def set_freqs(self, x, detun=1):\n    \"\"\"\n    Function to set the drive frequency and the detuning of the strobing\n\n    This function doesn't turn on the strobing.\n\n    Args:\n        freq (float): The drive frequency\n        detun (float): The detuning of the strobe\n    \"\"\"\n    self.hf2.daq.setDouble(\"/dev1224/oscs/0/freq\", x)\n    self.rigol.frequency = x + detun\n</code></pre>"},{"location":"programs/python_module_reference/#strobing_interferometer.acquisition.InstrumentManager.strobe_at","title":"<code>strobe_at(detun=1.0)</code>","text":"<p>Function that retrieves the drive frequency and set the strobe frequency</p> <p>This function doesn't turn on the strobing.</p> <p>Parameters:</p> Name Type Description Default <code>detun</code> <code>float</code> <p>The strobe detuning (positive means strobing at higher frequency)</p> <code>1.0</code> Source code in <code>strobing_interferometer/acquisition.py</code> <pre><code>def strobe_at(self, detun: float = 1.0):\n    \"\"\"\n    Function that retrieves the drive frequency and set the strobe frequency\n\n    This function doesn't turn on the strobing.\n\n    Args:\n        detun (float): The strobe detuning (positive means strobing at higher frequency)\n    \"\"\"\n    f = self.hf2.daq.getDouble(\"/dev1224/oscs/0/freq\")\n    self.rigol.frequency = f + detun\n</code></pre>"}]}